{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入库和config\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from utls import shuffle_and_split,Bag,Dataloader,Ngram\n",
    "import layer\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# configs：None不使用，否则覆盖下面块的config\n",
    "ONLY_LONG_SENTENCE = None       #只使用整句(True)还是也使用句子片段\n",
    "\n",
    "TESTSET_RATE = None             #测试集比例\n",
    "VALIDSET_RATE = None            #验证集比例\n",
    "TRAINSET_RATE = None            #训练集比例\n",
    "\n",
    "VOCAB_LENGTH = None             #bow或Ngram的词典最大总长度\n",
    "USE_NGRAM = None                #是否使用Ngram\n",
    "MAXN = None                     #ngram的最大N\n",
    "\n",
    "BATCH_SIZE = None               #训练、验证、测试集的batchsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "流程：\n",
    "\n",
    "1.处理数据<br>\n",
    "2.分割数据集<br>\n",
    "3.初始化word2vec<br>\n",
    "4.用数据集和word2vec设置dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取数据，并简单处理\n",
    "#只使用句子还是也使用句子片段\n",
    "only_long_sentence = False if ONLY_LONG_SENTENCE==None else ONLY_LONG_SENTENCE  \n",
    "\n",
    "\n",
    "#读取数据\n",
    "with open(r'.\\data\\train.tsv') as f:\n",
    "    tsvreader = csv.reader(f, delimiter='\\t')\n",
    "    temp = list(tsvreader)\n",
    "print(\"文件预览：\")\n",
    "print(temp[:5])\n",
    "print(temp[-5:])\n",
    "\n",
    "#裁剪\n",
    "data = temp[1:]\n",
    "if only_long_sentence:\n",
    "    print(\"config: 只保留完整句子\")\n",
    "    data = [data[i] for i in range(len(data)) \n",
    "            if (i==0 or data[i][1]!=data[i-1][1])]\n",
    "else:\n",
    "    print(\"config: 使用全部句子\")\n",
    "\n",
    "#文字转数值\n",
    "for i in range(len(data)):\n",
    "    data[i][0] = int(data[i][0])\n",
    "    data[i][1] = int(data[i][1])\n",
    "    data[i][3] = int(data[i][3])\n",
    "\n",
    "print(\"data预览：\")\n",
    "print(*data[:3],sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 数据集划分\n",
    "testset_rate = 0.2 if TESTSET_RATE==None else TESTSET_RATE\n",
    "trainset_rate = 0.7*0.8 if TRAINSET_RATE==None else TRAINSET_RATE\n",
    "validset_rate = 0.3*0.8 if VALIDSET_RATE==None else VALIDSET_RATE\n",
    "\n",
    "\n",
    "test_data, temp = shuffle_and_split(data,\n",
    "                        testset_rate/(testset_rate+trainset_rate+validset_rate))\n",
    "train_data, valid_data = shuffle_and_split(temp,\n",
    "                        trainset_rate/(trainset_rate+validset_rate)) \n",
    "\n",
    "data_set = (train_data, test_data, valid_data)\n",
    "print(\"test_data length:\",len(test_data))\n",
    "print(\"train_data length:\",len(train_data))\n",
    "print(\"valid_data length:\",len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建word2vec\n",
    "vocab_length = 400 if VOCAB_LENGTH==None else VOCAB_LENGTH\n",
    "use_ngram = False if USE_NGRAM==None else USE_NGRAM\n",
    "maxN = 2 if MAXN==None else MAXN\n",
    "\n",
    "\n",
    "word2vec, inputlen = None,None\n",
    "if not use_ngram:\n",
    "    #使用bow\n",
    "    sentences_train = [data[2] for data in train_data]\n",
    "    word2vec = Bag(sentences_train, vocab_length)\n",
    "    inputlen = word2vec.get_vocab_size()\n",
    "else:\n",
    "    #使用ngram\n",
    "    sentences_train = [data[2] for data in train_data]\n",
    "    gram_vocab_len = vocab_length//maxN\n",
    "    word2vec = Ngram(sentences_train, maxN, gram_vocab_len)\n",
    "    inputlen = word2vec.get_vocab_size()\n",
    "\n",
    "print(\"使用bow\" if not use_ngram else \"使用Ngram\")\n",
    "print(\"词表总长度为:\",inputlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 设置dataloader\n",
    "batch_size = 1000 if BATCH_SIZE==None else BATCH_SIZE\n",
    "\n",
    "\n",
    "input_transformer = [\n",
    "    word2vec.trans_to_tensor,\n",
    "    lambda x:np.reshape(x,(x.shape[0],1,-1))\n",
    "                   ]\n",
    "label_transformer = [lambda x:np.array(x,dtype=np.int32)]\n",
    "\n",
    "input_train = [data[2] for data in train_data]\n",
    "label_train = [data[3] for data in train_data]\n",
    "debug_train = [data[0] for data in train_data]\n",
    "\n",
    "input_valid = [data[2] for data in valid_data]\n",
    "label_valid = [data[3] for data in valid_data]\n",
    "debug_valid = [data[0] for data in valid_data]\n",
    "\n",
    "input_test = [data[2] for data in test_data]\n",
    "label_test = [data[3] for data in test_data]\n",
    "\n",
    "train_loader = Dataloader(batch_size, \n",
    "                          input_train, label_train, debug_train,\n",
    "                          input_transformer, label_transformer)\n",
    "valid_loader = Dataloader(batch_size, \n",
    "                          input_valid, label_valid, debug_valid,\n",
    "                          input_transformer, label_transformer)\n",
    "test_loader = Dataloader(batch_size, input_test, label_test,\n",
    "                         input_transform=input_transformer, label_transform=label_transformer)\n",
    "\n",
    "print(\"      batch数量, batch大小\")\n",
    "print(\"train:\",len(train_loader),train_loader._batchsize())\n",
    "print(\"valid:\",len(valid_loader),valid_loader._batchsize())\n",
    "print(\"test :\",len(test_loader),test_loader._batchsize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 模型\n",
    "input_lenght = inputlen\n",
    "output_length = 5\n",
    "param = None\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    \"\"\"耦合度较高，必须先forward，再getloss，再backward\"\"\"\n",
    "    def __init__(self,input,output,params=None):\n",
    "        self.linear = layer.Linear(input,output)\n",
    "        self.softmax = layer.SoftmaxAndCrossEntropy()\n",
    "        self.input_len = input\n",
    "\n",
    "        if params:\n",
    "            self.linear.load_param(*params)\n",
    "        else:\n",
    "            self.linear.init_param()\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"返回softmax以后的\"\"\"\n",
    "        x = self.linear.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "    def getloss(self,label):\n",
    "        \"\"\"返回loss\"\"\"\n",
    "        return self.softmax.get_loss(label)\n",
    "    def backward(self, learning_rate):\n",
    "        \"\"\"根据learning_rate反向传播\"\"\"\n",
    "        mid_stream = self.softmax.backward()\n",
    "        mid_stream = self.linear.backward(mid_stream)\n",
    "        self.linear.update_param(learning_rate)\n",
    "\n",
    "model = MyLinearModel(input_lenght,output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 训练\n",
    "epoch = 300\n",
    "lr = 0.03\n",
    "valid_interval = 200\n",
    "display_interval = 50\n",
    "\n",
    "\n",
    "trainnum = 0\n",
    "for ep in range(epoch):\n",
    "    batch_num = len(train_loader)\n",
    "    for i in range(batch_num):\n",
    "        input_tensor, lable, _ = train_loader[i]\n",
    "        soft_outp = model.forward(input_tensor)\n",
    "        model.getloss(lable)\n",
    "        model.backward(lr)\n",
    "        trainnum+=1\n",
    "        if trainnum%display_interval == 0:\n",
    "            soft_outp = soft_outp.reshape(-1,soft_outp.shape[-1])\n",
    "            print(\"batch:\",i)\n",
    "            print(\"softmax的结果:\",soft_outp[:5],sep='\\n')\n",
    "            print(\"softmax预测:\",np.argmax(soft_outp[:5],axis=1))\n",
    "            print(\"lable:\",lable[:5])\n",
    "\n",
    "            ans = np.argmax(soft_outp,axis=1)\n",
    "            right = np.sum(ans==lable)\n",
    "            print(\"accuracy: \",right/ans.shape[0])\n",
    "        if trainnum%valid_interval == 0:\n",
    "            valid_batch_num = len(valid_loader)\n",
    "            valid_right_num = 0\n",
    "            for i in range(valid_batch_num):\n",
    "                input_tensor, lable, debug = valid_loader[i]\n",
    "                soft_outp = model.forward(input_tensor)\n",
    "                ans = np.argmax(soft_outp.reshape(-1,soft_outp.shape[-1]), axis=1)\n",
    "                valid_right_num += np.sum(ans == lable)\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"验证Accuracy:\", \n",
    "                valid_right_num/(len(valid_loader)*valid_loader._batchsize()))\n",
    "            print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 测试\n",
    "test_batch_num = len(test_loader)\n",
    "test_accuracy = 0\n",
    "for i in range(test_batch_num):\n",
    "    input_tensor, lable, debug = test_loader[i]\n",
    "    soft_outp = model.forward(input_tensor)\n",
    "    ans = np.argmax(soft_outp.reshape(-1,soft_outp.shape[-1]), axis=1)\n",
    "    test_accuracy += np.sum(ans == lable)\n",
    "print(\"测试Accuracy:\", \n",
    "      test_accuracy/(len(test_loader)*test_loader._batchsize()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo5use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
